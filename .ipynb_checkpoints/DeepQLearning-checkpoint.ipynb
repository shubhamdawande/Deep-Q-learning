{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CZPMLZDFR8V"
   },
   "outputs": [],
   "source": [
    "## Google Colab Setup\n",
    "## Get Google SDK Key\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3Vi2kwKp7RW"
   },
   "outputs": [],
   "source": [
    "## Mount Google Drive\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "nGklc4GYp-r6",
    "outputId": "0cab0b07-bb08-4a7a-8f36-aa0ec891c75e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/Colab_Notebooks\r\n",
      "['', '/env/python', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/local/lib/python2.7/dist-packages/IPython/extensions', '/content/.ipython', '/content/drive/Colab_Notebooks/']\n"
     ]
    }
   ],
   "source": [
    "## Set up Current DIR, SYS PATH\n",
    "import os\n",
    "os.chdir('drive/Colab_Notebooks/')\n",
    "!pwd\n",
    "import sys\n",
    "sys.path.append('/content/drive/Colab_Notebooks/')\n",
    "print sys.path\n",
    "\n",
    "## Colab setup done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "TMOhFUYYjRnk",
    "outputId": "83eee88b-b678-4d3f-f6dd-3e9af69d3f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Episode: 0', 'Total reward: 30.0', 'epsilon: 0.9959', 'Training Loss 1.5570')\n",
      "Model Saved\n",
      "('Episode: 1', 'Total reward: 155.0', 'epsilon: 0.9896', 'Training Loss 3.8456')\n",
      "('Episode: 2', 'Total reward: 155.0', 'epsilon: 0.9832', 'Training Loss 0.3932')\n",
      "('Episode: 3', 'Total reward: 180.0', 'epsilon: 0.9770', 'Training Loss 15.8457')\n",
      "('Episode: 4', 'Total reward: 105.0', 'epsilon: 0.9707', 'Training Loss 3.4705')\n",
      "('Episode: 5', 'Total reward: 75.0', 'epsilon: 0.9669', 'Training Loss 0.0494')\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "## Here we use Q learning to play Gym-Atari Space Invadors game.\n",
    "## We take a frame(1 out of 4 consecutive frames) at each time step as input.\n",
    "## Q learning algorithm is implemented using a Deep Convolutional Network with 3 Convolutional layers(no pooling)\n",
    "## and a fully connected layer\n",
    "## Change training=False to test the learnt model\n",
    "\n",
    "# import dependencies\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from skimage import color\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import warnings\n",
    "try:\n",
    "    import gym\n",
    "except:\n",
    "    !pip install gym\n",
    "    import gym\n",
    "try:\n",
    "    import atari_py\n",
    "except:\n",
    "    !pip install cmake\n",
    "    !pip install gym[atari]\n",
    "    import atari_py\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Initialize environment\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "\n",
    "############ Tuning parameters ############\n",
    "\n",
    "training = True\n",
    "num_episodes = 6\n",
    "num_steps = 50000\n",
    "batch_size = 64\n",
    "\n",
    "# large epsilon->exploration, small epsilon->exploitation\n",
    "epsilon = 1.0\n",
    "epsilon_max = 1.0 \n",
    "epsilon_min = 0.01\n",
    "decay_rate = 0.00001\n",
    "\n",
    "stack_len = 4\n",
    "state_size = [110, 84, stack_len] # downsized for lower computation time\n",
    "action_size = env.action_space.n\n",
    "learning_rate = 0.00025\n",
    "gamma = 0.9\n",
    "\n",
    "memory_size = 1000000\n",
    "pretrain_len = batch_size\n",
    "possible_actions = np.identity(action_size) # convert actions to one-hot vectors\n",
    "\n",
    "############ Frame preprocessing ############\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame_gray       = color.rgb2gray(frame)   # convert to grayscale [210,160,1]\n",
    "    frame_normalized = frame_gray/255.0        # normalize image in 0 to 1\n",
    "    frame_resized    = resize(frame_normalized, [110,84]) \n",
    "    return frame_resized\n",
    "\n",
    "############ Stack frames in a deque of length 4############\n",
    "\n",
    "stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_len)], maxlen=4)\n",
    "\n",
    "def update_stack(frame, stacked_frames, episode_start):\n",
    "  \n",
    "    frame = preprocess_frame(frame)\n",
    "\n",
    "    if episode_start==True:\n",
    "        stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_len)], maxlen=4)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stack = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame) # add frame t\n",
    "        stack = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "  # Stack: input to network(shape:[110,84,stack_len])\n",
    "    return stack, stacked_frames \n",
    "\n",
    "############  Memory class ############\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "      buffer_size = len(self.buffer)\n",
    "      index = np.random.choice(np.arange(buffer_size), size = batch_size, replace = False)\n",
    "      return [self.buffer[i] for i in index]\n",
    "\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "############ Initialize memory ############\n",
    "\n",
    "for i in range(pretrain_len):\n",
    "    if i==0:\n",
    "        frame = env.reset()\n",
    "        stack, stacked_frames = update_stack(frame, stacked_frames, True)\n",
    "  \n",
    "    action = env.action_space.sample()\n",
    "    next_frame, reward, done, info = env.step(action)\n",
    "    next_stack, stacked_frames = update_stack(next_frame, stacked_frames, False)\n",
    "    action = possible_actions[action]\n",
    "\n",
    "    if done: # if game is finished\n",
    "        done = False\n",
    "        next_stack = np.zeros(stack.shape)\n",
    "        memory.add((stack, action, reward, next_stack, done*1))\n",
    "        frame = env.reset()\n",
    "        stack, stacked_frames = update_stack(frame, stacked_frames, True)\n",
    "    else:\n",
    "        memory.add((stack, action, reward, next_stack, done*1))\n",
    "        stack = next_stack\n",
    "\n",
    "############ Build our Network ############\n",
    "\n",
    "class DeepQNetwork:\n",
    "  \n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQN'):\n",
    "    \n",
    "        self.state_size  = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "        with tf.variable_scope(name):\n",
    "            # Input/Output placeholders\n",
    "            self.x  = tf.placeholder(tf.float32, [None, state_size[0], state_size[1], state_size[2]], name='input') # state input: batches x h x w x 4\n",
    "            self.actions = tf.placeholder(tf.float32, [None, self.action_size], name='actions') # action input: batches x action_size\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name='target_Q')\n",
    "    \n",
    "            # 3 convolution layers\n",
    "            self.layer1 = self.conv_layer(self.x, self.state_size[2], 32, [8,8], name='layer1')\n",
    "            self.layer2 = self.conv_layer(self.layer1, 32, 64, [4,4], name='layer2')\n",
    "            self.layer3 = self.conv_layer(self.layer2, 64, 64, [3,3], name='layer3')\n",
    "      \n",
    "            # Flattening\n",
    "            self.flattened = tf.contrib.layers.flatten(self.layer3)\n",
    "      \n",
    "            self.layer4 = tf.layers.dense(inputs = self.flattened, units=512, activation=tf.nn.elu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                    name=\"layer4\")\n",
    "            \n",
    "            self.y_ = tf.layers.dense(inputs = self.layer4, kernel_initializer=tf.contrib.layers.xavier_initializer(), units=self.action_size, \n",
    "                                           activation=None)\n",
    "    \n",
    "            self.pred_Q = tf.reduce_sum(tf.multiply(self.y_, self.actions))\n",
    "\n",
    "            # Loss function\n",
    "            self.squared_loss = tf.reduce_mean(tf.square(self.target_Q - self.pred_Q)) # squared sum over batches\n",
    "\n",
    "            # Optimizer\n",
    "            self.adam_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.squared_loss)\n",
    "\n",
    "    ## Template for convolution+pooling layer\n",
    "    def conv_layer(self, input_data, num_input_channels, num_filters, conv_filter_shape, name):\n",
    "  \n",
    "        conv_filter_size = [conv_filter_shape[0], conv_filter_shape[1], num_input_channels, num_filters] \n",
    "  \n",
    "        # Conv layer\n",
    "        kernel = tf.get_variable(\n",
    "               initializer=None,#tf.contrib.layers.xavier_initializer_conv2d(), \n",
    "               shape=conv_filter_size,\n",
    "               name=name+'_kernel')\n",
    "        conv_output  = tf.nn.conv2d(input_data, kernel, [1,2,2,1], padding='VALID')\n",
    "        conv_output  = tf.nn.elu(conv_output)\n",
    "  \n",
    "        return conv_output\n",
    "\n",
    "tf.reset_default_graph()\n",
    "DeepQNetwork = DeepQNetwork(state_size, action_size, learning_rate) # instantiate class\n",
    "\n",
    "# TensorBoard Setup\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/dqn/2\")\n",
    "#writer.add_graph(sess.graph)\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.squared_loss)\n",
    "write_op = tf.summary.merge_all()  \n",
    "\n",
    "############ Training ############\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if training == True:\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        reward_list = []\n",
    "    \n",
    "        for episode in range(num_episodes):\n",
    "      \n",
    "            # Get initial state (->rgb frame)\n",
    "            frame = env.reset()\n",
    "            # Add frame to stack\n",
    "            stack, stacked_frames = update_stack(frame, stacked_frames, True)\n",
    "            step = 0\n",
    "            episode_reward = 0\n",
    "            total_reward = 0\n",
    "\n",
    "            while step < num_steps:\n",
    "        \n",
    "                step += 1\n",
    "                #print 'step:', step\n",
    "                ## Choose optimal action using epsilon greedy strategy\n",
    "                random_number = random.uniform(0,1)\n",
    "                decay_step += 1\n",
    "                epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-decay_rate * decay_step)\n",
    "                if random_number < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    k1,k2,k3 = stack.shape\n",
    "                    action = np.argmax(sess.run(DeepQNetwork.y_, feed_dict={DeepQNetwork.x: stack.reshape(1,k1,k2,k3)})) # input size: [batches,state_size]\n",
    "\n",
    "                ## Perform the action and update stack with next frame\n",
    "                next_frame, reward, done, info = env.step(action) # next_frame: HxWx3 \n",
    "                action = possible_actions[action]\n",
    "\n",
    "                # Rewards\n",
    "                episode_reward += reward \n",
    "        \n",
    "                # Append to memory\n",
    "                if done:\n",
    "                    done = False\n",
    "                    next_frame = np.zeros((state_size[0], state_size[1]),  dtype=np.int)\n",
    "                    next_stack, stacked_frames = update_stack(next_frame, stacked_frames, False)\n",
    "                    step = num_steps\n",
    "                    memory.add((stack, action, reward, next_stack, done*1))\n",
    "                    total_reward = episode_reward\n",
    "                    reward_list.append([episode, episode_reward])\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                          'epsilon: {:.4f}'.format(epsilon),\n",
    "                          'Training Loss {:.4f}'.format(squared_loss))\n",
    "                else:\n",
    "                    next_stack, stacked_frames = update_stack(next_frame, stacked_frames, False) # stack: HxWx4\n",
    "                    memory.add((stack, action, reward, next_stack, done*1))\n",
    "                    stack = next_stack\n",
    "\n",
    "                ## Learning the network weights\n",
    "                mini_batch       = memory.sample(batch_size)\n",
    "                stack_batch      = np.array([batch[0] for batch in mini_batch], ndmin=3)\n",
    "                action_batch     = np.array([batch[1] for batch in mini_batch])\n",
    "                reward_batch     = np.array([batch[2] for batch in mini_batch])\n",
    "                next_stack_batch = np.array([batch[3] for batch in mini_batch])\n",
    "                done_batch       = np.array([batch[4] for batch in mini_batch]).reshape(batch_size,1)\n",
    "                target_Q_batch = []\n",
    "        \n",
    "                # Find max Q values from all actions for next state in a mini batch \n",
    "                next_Q_batch = sess.run(DeepQNetwork.y_, feed_dict={DeepQNetwork.x: next_stack_batch})\n",
    "\n",
    "                for j in range(batch_size):\n",
    "          \n",
    "                  if done_batch[j] == 1:\n",
    "                      target_Q_batch.append(reward_batch[j])\n",
    "                  else:\n",
    "                      target_Q_batch.append(reward_batch[j] + gamma*np.max(next_Q_batch[j]))\n",
    "\n",
    "                target_Q = np.array([k for k in target_Q_batch])\n",
    "                squared_loss, _ = sess.run([DeepQNetwork.squared_loss, DeepQNetwork.adam_optimizer], \n",
    "                                            feed_dict={DeepQNetwork.x: stack_batch, \n",
    "                                                       DeepQNetwork.target_Q: target_Q, \n",
    "                                                       DeepQNetwork.actions: action_batch})        \n",
    "        \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DeepQNetwork.x: stack_batch,\n",
    "                                                    DeepQNetwork.target_Q: target_Q,\n",
    "                                                    DeepQNetwork.actions: action_batch})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "        \n",
    "        # Save model every 5 episodes\n",
    "        if episode % 5 == 0:\n",
    "            save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "('EPISODE ', 0)\n",
      "('Score', 130.0)\n",
      "('EPISODE ', 1)\n",
      "('Score', 65.0)\n",
      "('EPISODE ', 2)\n",
      "('Score', 155.0)\n",
      "('EPISODE ', 3)\n",
      "('Score', 70.0)\n",
      "('EPISODE ', 4)\n",
      "('Score', 275.0)\n",
      "('EPISODE ', 5)\n",
      "('Score', 270.0)\n",
      "('EPISODE ', 6)\n",
      "('Score', 400.0)\n",
      "('EPISODE ', 7)\n",
      "('Score', 35.0)\n",
      "('EPISODE ', 8)\n",
      "('Score', 585.0)\n",
      "('EPISODE ', 9)\n",
      "('Score', 45.0)\n",
      "203.0\n"
     ]
    }
   ],
   "source": [
    "## Test the model\n",
    "## After testing for 100 episodes our model can play game quite reasonably achieving average score of 203\n",
    "\n",
    "############ Testing ############\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        total_test_rewards = []\n",
    "\n",
    "        # Load the model\n",
    "        saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "        for episode in range(10):\n",
    "            total_rewards = 0\n",
    "\n",
    "            frame = env.reset()\n",
    "            stack, stacked_frames = update_stack(frame, stacked_frames, True)\n",
    "\n",
    "            print(\"EPISODE \", episode)\n",
    "            step = 0\n",
    "            while True:\n",
    "\n",
    "                # Reshape the state\n",
    "                stack = stack.reshape((1, state_size[0], state_size[1], state_size[2]))\n",
    "                # Get action from Q-network \n",
    "                # Estimate the Qs values state\n",
    "                Q_value = sess.run(DeepQNetwork.y_, feed_dict = {DeepQNetwork.x: stack})\n",
    "\n",
    "                # Take the biggest Q value (= the best action)\n",
    "                action = np.argmax(Q_value)\n",
    "\n",
    "                #Perform the action and get the next_state, reward, and done information\n",
    "                next_frame, reward, done, _ = env.step(action)\n",
    "                #env.render()\n",
    "\n",
    "                action = possible_actions[action]\n",
    "                total_rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    print (\"Score\", total_rewards)\n",
    "                    total_test_rewards.append(total_rewards)\n",
    "                    break\n",
    "\n",
    "                next_stack, stacked_frames = update_stack(next_frame, stacked_frames, False)\n",
    "                stack = next_stack\n",
    "\n",
    "        print np.mean(total_test_rewards)        \n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoMhnLs-ae_u"
   },
   "outputs": [],
   "source": [
    "## Launch Tensorboard\n",
    "!tensorboard --logdir=./tensorboard/dqn/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EIIbuZ2TMKp"
   },
   "outputs": [],
   "source": [
    "# Check GPU RAM allocation\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepQLearning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
